\chapter{Analysis and Optimization}\label{ch:analysis_and_optimization}

	In this chapter we look at the static analyses performed on the PLR syntax 
	tree before compilation, these include some classical dataflow analyses 
	as well as some analyses that are more specific to the process algebra 
	domain. The results of these analyses can be used to optimize the 
	compilation process, the optimizations are presented and explained, and 
	their implications for debugging support are discussed.
	
\section{Analysing process algebra}
	
	\subsection{Traditional data flow analysis}
	Many of the most useful static analyses that compilers perform are based on 
	\textit{data flow analysis}. For these types of analyses it is necessary to 
	build up a \textit{control flow graph} of the program being analysed. The 
	control flow graph shows which program points lead directly to which other 
	program points, in some cases the reverse control flow graph is needed, 
	which shows for a program point $p$ what its immediate predecessors are. The 
	following snippet of typical imperative code is an example:
	\begin{Exa}
	\label{ex:control_flow}
	\begin{verbatim}
	
                     1: y := 2
                     2: x := 1
                     3: while x < 6 do
                     4: 	  x := x + 1   
                        od
                     5: print x
	\end{verbatim}
	\end{Exa}
	
	In this example the program points are labelled from 1-5. Program points are 
	those points in the program where something happens, expression are 
	evaluated, variables are assigned, etc. The control flow for this example 
	would be (1,2), (2,3), (3,4), (3,5), (4,3) and (4,5). The analysis is then 
	typically performed by having each program point have an input set and an 
	output set, the input set represents the state of the program as the point 
	is reached and is based on the state of its predecessors, the output set 
	represents the state of the program after the program point has been 
	evaluated, and is based on the points input set with some modifications 
	based on what happened at the program point. 
	
	To give a concrete example suppose we have an analysis which is determining 
	for the code snippet in Example~\ref{ex:control_flow} which variables have 
	been assigned at each point in the program. For the program point labeled 1 
	(x := 1), its input set would be $\{y\}$ as $y$ is the only variable that 
	has been assigned at that point. Since program point 1 assigns to the 
	variable $x$ then its output set would be the union of its input set and 
	$\{x\}$, or $\{x,y\}$. The functions used to modify the input set to create 
	the output set are commonly called \textit{Kill} and \textit{Gen}, the 
	\textit{Kill} function removes items from the input set and the \textit{Gen} 
	function adds new items to the output set. This can be shown as (for program 
	point $p$):
	
	\begin{center}
	$p_{output} = p_{input}\ \cup\ Gen(p)\ \setminus\ Kill(p) $
	\end{center}
	
	To get the final result of the analysis these calculations must be repeated 
	for each point in the program until the input and output sets of each become 
	stable. The result is an approximation, either over approximation (something 
	\textit{may} happen on the path to the program point) or an 
	under approximation (something \textit{must} have happened on the path to 
	the program point).  To calculate the input and output sets of each program 
	points an iterative worklist algorithm is used. There are many variations of 
	these algorithms, they keep track of which program points change and which 
	other program points must then be re-calculated. There is a lot more to data 
	flow analysis than explained here, for instance whether output sets of 
	predecessors are combined using the union or intersection operator, and what 
	the initial content of the input sets are, but we will not go into more 
	detail on how data flow analysis generally works here, this is meant only as 
	a brief introduction before explaining the analyses performed in the PLR. A 
	more formal introduction to the subject of data flow analysis can be found 
	in \cite{program_analysis}.
	
	\subsection{Data flow analysis in process algebra}
	
	Two properties of process algebra make it different from imperative 	
	languages when it comes to data flow analysis.
	
	\begin{enumerate}
		\item There are no looping constructs. The reason that data flow analysis 
		on imperative languages needs iterative worklist algorithms is because of 
		looping. When looping is not a part of the language then the whole program 
		can be analyzed from top to bottom (or bottom to top) in one pass, each 
		program point only needs to be calculated once.
		
		\item There are branches, but they are never re-joined later in the 
		program. This implies that at every point in the program is is possible to 
		know exactly what path was taken to get to that point. Note that this only 
		holds for forward analysis. Backwards analysis can be seen as multiple 
		branches joining, and so in a backward analysis it is not possible to know 
		at program point $p$ from which branch a particular item in $p$'s input 
		set originates.
		
	\end{enumerate}

	These two properties might not hold for all process algebras in existence, 
	but they certainly hold for both CCS and KLAIM, as well as some other 
	prominent algebras such as CSP, and they do simplify the implementation of 
	these analyses for process algebra. Sections \ref{live_variables} and 
	\ref{reaching_definitions} discuss one backward and one forward data flow 
	analysis and how they were implemented in an extensible way for the PLR.
	
\section{Analyses}
	
	The analyses presented here generally follow the same pattern. They implement
	an \code{IAnalysis} interface and inherit from \code{AbstractVisitor}. 
	They traverse the syntax tree to analyze it and issue warnings about any
	anomalies found. In some cases they mark certain syntax nodes as not being
	used, the compilation then uses that information for optimization, for 
	example to skip compiling certain things that are guaranteed never to be 
	executed. These optimizations can be turned on and off in the compile 
	options that are passed to the PLR. The optimizations and debugging support 
	are mutually exclusive, this is due to the fact that the sequence points in 
	an optimized executable may not match the actual source code file any 
	longer, which could make the debugging rather confusing.
	
\subsection{Live Variables}\label{live_variables}

  \textit{Live Variables Analysis} is a classic data flow analysis. Its 
  purpose is to identify at each program point which variables are 
  \textit{live}, that is which variables will be read later on in the program 
  in the paths that follow the program point in question. This is a backward 
  analysis and is traditionally used to identify assignments to variables that 
  have no effect, for instance if the variable $x$ is assigned at program 
  point $p$, but along all paths that follow $p$ the variable is either never 
  read, or assigned to again before being read then the assignment at $p$ has 
  no effect and can be eliminated. The analysis is an over-approximation, we 
  cannot safely eliminate the assignment to $x$ if there \textit{may} be a 
  path after $p$ where $x$ is read. This implies that the input set of $p$ 
  will be the \textit{union} of the output sets of its pre-decessors (here the 
  pre-decessors actually mean the program points that follow $p$ as it is a 
  backward analysis).
  
  In the analysis of the PLR syntax tree, a process is considered a program 
  point. Additionally, syntax nodes that represent a process constant being 
  defined are considered program points, as they may contain defining 
  occurrences of variables. For instance, in $ CS(x,y) \defeq a . 0$ we would 
  consider $CS(x,y)$ a program point, since it defines the variables $x$ and 
  $y$, and it can be beneficial at that point to know whether the initial 
  value of those variables was ever used in the process. We have two helper 
  functions, \textit{assigned(x)} takes in an action and returns a set of the 
  variables assigned in that action. An action in this case can be the 
  receiving of values through a channel or sending values through a channel. 
  The other helper function, \textit{read(x)} takes in an expression and 
  returns all variables evaluated in that expression, or takes in an action 
  and returns all variables evaluated in that action.
  Figure~\ref{fig:read_assigned} shows some examples of the use of 
  \textit{read} and \textit{assigned}, Figure~\ref{fig:killgen_livevariables} 
  shows how the \textit{Kill} and \textit{Gen} functions are defined for each 
  of the process types in the PLR syntax tree.
  
	\begin{figure}[h!]
	\begin{ARRAY}{r l l}
  assigned(\mathrm{channel}(x,y)) & = & \{x,y\} \vspace{5pt}\\
  read(\overline{(\mathrm{channel}(z+1,a-b)}) & = & \{z,a,b\} \vspace{5pt}\\
  read(x+1/z) & = & \{x,z\} \\
	\end{ARRAY}
	\caption{Examples of the \textit{read} and \textit{assigned} functions}\label{fig:read_assigned}
	\end{figure}
  
  \begin{figure}[ht!]
  
  \begin{ARRAY}{r l l}
  		
  		Kill(a.P) & = & assigned(a) \\
  		Gen(a.P) & =& read(a) \vspace{7pt}\\
      
      Kill(P \mid Q) & = & \emptyset \\
      Gen(P \mid Q) & = & \emptyset \vspace{7pt}\\
   
      Kill(P + Q) & = & \emptyset \\
      Gen(P + Q) & = & \emptyset \vspace{7pt}\\
  
      Kill(0) & = & \emptyset \\
      Gen(0) & = & \emptyset \vspace{7pt}\\
      
      Kill(\mathrm{if}\ bexp\ \mathrm{then}\ P\ \mathrm{else}\ Q) & = & \emptyset\\
      Gen(\mathrm{if}\ bexp\ \mathrm{then}\ P\ \mathrm{else}\ Q) & = & read(bexp)\vspace{7pt}\\

      (\mathrm{process\ definitions}) & &\\
      Kill(K(x,y,z) \defeq) & = & \{x,y,z\} \\
      Gen(K(x,y,z) \defeq) & = & \emptyset \vspace{7pt}\\

      (\mathrm{process\ invocations}) & &\\
      Kill(K(exp_1,...,exp_n)) & = & \emptyset \\
      Gen(K(exp_1,...,exp_n)) & = & read(exp_1) \cup ... \cup\ read(exp_n) \\

  
  \end{ARRAY} 
\caption{\textit{Kill} and \textit{Gen} functions for Live Variables}\label{fig:killgen_livevariables}  
  \end{figure}
  
  The actual implementation of the \textit{assigned()} and \textit{read()} 
  functions is done with two properties that are present on all syntax nodes
  that derive from either \code{Process} or \code{Action}. These properties
  are \code{AssignedVariables} and \code{ReadVariables} and return the
  assigned and read variables of a process or action. Having these as 
  properties of the syntax nodes instead of as part of the implementation of 
  the analysis allows for greater extensibility. Languages implemented using 
  the PLR that have their own custom syntax nodes simply need to override 
  these properties and can then use the analysis without further 
  modifications. This is the case in the KLAIM implementation discussed in 
  Chapter~\ref{ch:klaim}, it has its own custom actions and a custom process 
  type and they simply implement these properties.
  
  Constructing the flow graph is trivial, since there are no loops or joining
  branches. In fact, a special flow graph is not constructed, instead the PLR
  syntax tree is used directly. To make it simpler, each \code{Process} node
  is required to implement a \code{FlowsTo} property which is a list of all
  the processes it can flow to. For an \texttt{if-else} process these are the 
  if and else branches, for parallel composition these are the composed 
  processes, etc. In addition each node in the syntax tree has a 
  \code{Parent} property which references its parent and can be used as a 
  reverse flow graph. Again, having these properties directly on the syntax 
  tree is useful so that additional process types can be added without having 
  to modify the analysis code. The input and output sets are stored in a a 
  \code{Tag} property which is also on each syntax node, this is a generic 
  object reference which analyses may use to temporarily store data associated 
  with each node.
  
  Instead of an iterative worklist algorithm the analysis itself uses the
  visitor pattern discussed in Section~\ref{sec:visitor}. It simply inherits
  from \code{AbstractVisitor} and overrides the \code{Visit(Process p)} 
  method which ensures that it will be called for every node in the syntax 
  tree that inherits from \code{Process}. The visitor does a depth-first 
  recursive traversal of  the syntax tree, the \code{Visit} is called on 
  child nodes before parent nodes so whenever we are processing a process $p$ 
  we know that all its child nodes have already been processed. What is needed
  in the method itself is then:
  
  	\begin{enumerate}
  		\item Construct the input set of $p$ by joining the output sets of all 
  		the processes in its \code{FlowsTo} property.

  		\item Check whether any variable $x$ in \code{p.AssignedVariables} is 
  		not in $p$'s input set. If it is not, then the assignment at $p$ has no 
  		effect and a warning is issued, and an \code{IsUsed} property on $x$ 
  		is set to \texttt{false}. This can be used later for optimization during 
  		compilation.
  		
  		\item Construct the output set of $p$ by taking the input set and 
  		removing all variables that are in \code{p.AssignedVariables} and then 
  		adding all variables that are in \code{p.ReadVariables}.
  	
  	\end{enumerate}
  
  Once the analysis has been performed a number of warnings have been issued 
  and all assignments to variables that have no effect have been marked as not 
  used. In some cases it may be needed to assign to a variable that is never 
  used because the interface of another process requires it. For instance, one 
  process may send a value on channel $ch$, another process may wish to 
  synchronize but does not care about the value sent. In that case the 
  variable that is bound during the synchronization can be named 
  \textbf{notused} and then no warnings will be issued.
  
  During the compilation stage the PLR will look at every variable that is 
  assigned and check whether its \code{IsUsed} property is set to 
  \texttt{false}. If the assignment is never used then no bytecode is emitted
  to store a value in the variable and no variable is declared. If the 
  assignment is unused because another assignment is made before the variable 
  is read, then the variable is defined at that point instead of the original 
  point. Due to the way the PLR is constructed this may save one or more
  processes from having that variable as a field or a local variable, thus
  reducing the size of the compiled executable.

\subsection{Reaching Definitions}\label{reaching_definitions}

  \textit{Reaching Definitions} is another classic data flow analysis. Its 
  purpose is to identify which assignments may reach a given point in a 
  program.
  
\begin{Exa}
	\label{ex:reaching_definition}
	\begin{verbatim}
	
                     1: x := 1
                     2: x := 2
                     3: print x
	\end{verbatim}
\end{Exa}  
	
	In the example above the assignment at line 2 reaches line 3. The assignment 
	at line 1 reaches line 2 but not 3, since $x$ is assigned again at line 2. 
	So at line 3 we know that the value of $x$ is that which was assigned at 
	line 2. With branching and re-joining branches there may be more than one 
	definition of a particular variable that reach a certain point, for example 
	when a variable is assigned in both branches of an \texttt{if-then-else} 
	statement. However since this is a forward analysis and the PLR does not 
	have re-joining branches this cannot happen in the PLR's implementation of 
	this analysis. 
	
	Reaching Definitions analysis can be used for many common compiler 
	optimizations, such as constant propagation and common subexpression 
	elimination. However in the PLR it is used for only one thing, to detect the 
	use of unassigned variables. Once the analysis has been performed it is 
	possible to examine each program point $p$ and see whether all variables
	read at $p$ have entries in $p$'s input set. If a variable $x$ is evaluated
	at $p$ and does not exist in $p$'s input set, then there is no definition
	of $x$ that reaches $p$ and it is being used before being assigned. The 
	\textit{Kill} and \textit{Gen} functions are shown in 
	Figure~\ref{fig:killgen_reachingdefinitions}. Since the assignment to 
	variable $x$ defined at one point in the program is not the same as the 
	assignment to variable $x$ defined at another place in the program the 
	function \textit{named()} is used. ${Kill(a.P) = (named(assigned(a)))}$ 
	means to remove all the variables from the set which are named the same as 
	those that were assigned in $a$, whereas for the \textit{Gen} we have 
	$Gen(a.P) = assigned(a)$ which means to add the exact variables occuring in
	$a$ to the set. 
	
  \begin{figure}[ht!]
  \begin{ARRAY}{r l l}
  		
  		Kill(a.P) & = & named(assigned(a)) \\
  		Gen(a.P) & =& assigned(a) \vspace{7pt}\\
      
      Kill(P \mid Q) & = & \emptyset \\
      Gen(P \mid Q) & = & \emptyset \vspace{7pt}\\
   
      Kill(P + Q) & = & \emptyset \\
      Gen(P + Q) & = & \emptyset \vspace{7pt}\\
  
      Kill(0) & = & \emptyset \\
      Gen(0) & = & \emptyset \vspace{7pt}\\
      
      Kill(\mathrm{if}\ bexp\ \mathrm{then}\ P\ \mathrm{else}\ Q) & = & \emptyset\\
      Gen(\mathrm{if}\ bexp\ \mathrm{then}\ P\ \mathrm{else}\ Q) & = & \emptyset\vspace{7pt}\\

      (\mathrm{process\ definitions}) & &\\
      Kill(K(x,y,z) \defeq) & = & named(x,y,z) \\
      Gen(K(x,y,z) \defeq) & = & \{x,y,z\} \vspace{7pt}\\

      (\mathrm{process\ invocations}) & &\\
      Kill(K(exp_1,...,exp_n)) & = & \emptyset \\
      Gen(K(exp_1,...,exp_n)) & = & \emptyset\\

  
  \end{ARRAY}
  \caption{\textit{Kill} and \textit{Gen} functions for Reaching Definitions}\label{fig:killgen_reachingdefinitions}
  \end{figure}

	The implementation of Reaching Definitions is similar to that of Live 
	Variables. An analysis class inherits from \code{AbstractVisitor} and 
	overrides its \code{Visit(Process p)} method. Since this is a forward 
	analysis the traversal is a little different than in Live 
	Variables. Here, the property \code{VisitParentBeforeChildren} is set to 
	\texttt{true} on the \code{AbstractVisitor}. This makes sure that the tree 
	can be processed in forward order instead of backward. Then, in the method 
	the following steps are performed:
	
  	\begin{enumerate}
  		\item Check the input set of $p$ against the property 
  		\code{p.ReadVariables}. If there exists a variable in 
  		\code{p.ReadVariables} that has no entry in $p$'s input set then it is 
  		being used before assignment and a warning is issued.
  		
  		\item Create the input set of each process $P_i$ in \code{p.FlowsTo} 
  		by taking the input set of $p$ and adding all variables from 
  		\code{p.AssignedVariables}, replacing any previous entries for the 
  		same variable.
  	
  	\end{enumerate}
  	
  After the analysis has run the PLR checks whether it produced any warnings. 
  If it did then an error message to that effect is printed to the screen and 
  the compilation is aborted.
	

\subsection{Constant Expressions}

	A simple optimization that can be performed is to calculate the expressions 
	that can be fully evaluated at compile time and replace them with a constant 
	with the result of the calculation. Granted, it is unlikely that large 
	expressions can be computed at compile time, unless some constant 
	propagation is also performed, but it is nevertheless useful to compute 
	those simple ones that can. The implementation is simple (and was partially 
	shown in Figure~\ref{fig:expression_folder}), simply subclass an 
	\code{AbstractVisitor} and override a \code{Visit} method for each of 
	the binary expression nodes (arithmetic, relational and logical). Check if 
	both the left and right childnodes are constants, if they are then 
	calculate the result according to the operator stored in the binary node. 
	Finally replace the binary node with a new constant node that carries the 
	result of the computation. The same goes for the unary minus node.
	
	The one extra thing that this analysis does is that it also visits the
	\code{BranchProcess} node, which is the implementation of the
	\texttt{if-then-else} statement. If the boolean expression in the 
	\code{BranchProcess} has been fully computed then the analysis will set 
	the \code{IsUsed} flag to \code{false} on the branch that will never 
	been chosen, according to the result of the boolean expression. During the 
	compilation stage, if optimizations are turned on, the PLR will then replace 
	the \code{BranchProcess} with the branch that is guaranteed to be taken 
	and only compile that branch.

\subsection{Nil Process Warnings}

	\textit{Nil Process Warnings} is a simple analysis which looks for nil 
	processes in places where they are useless and can be eliminated. This 
	includes nil processes that are part of parallel composition ($P \mid 0 
	\equiv P$) and nil processes that are an option in non deterministic choice 
	(in $P+0$ the nil process will never be selected). This is implemented as a 
	visitor that visits all \code{NilProcess} nodes, checks whether their 
	parent nodes are \code{ParallelComposition} or 
	\code{NonDeterministicChoice}. If they are, then a warning is issued and 
	the property \code{IsUsed} on the nil process nodes is set to 
	\texttt{false}, which allows the compilation to skip those processes.
	
	The analysis also checks for process definitions that are defined as nothing 
	more than the nil process (e.g. $P \defeq 0$) and issues a warning that 
	states that any process invocation of $P$ can be replaced with a nil process.

\subsection{Unmatched Channels}

	When a process tries to synchronize on a channel that no other process 
	anywhere in the application ever synchronizes on then that process will be 
	blocked forever. That scenario is not hard to detect manually in a small 
	system, however once the system grows it becomes increasingly harder, 
	especially when we factor in that a channel can be relabeled multiple 
	times. 
	
	To attempt to detect these synchronizations which will never complete, the 
	PLR contains an analysis named \textit{Unmatched Channels}. It begins by
	collecting the channel names of all \code{InAction} and \code{OutAction}
	instances in the system. It then collects all the relabellings that can occur
	(although it does not take into account relabeling that is done by custom
	.NET methods). Every variation of channel names that are possible are then
	created using the relabellings, this is done on a global scale without 
	consideration for whether the relabeling has a chance of actually being
	applied to a particular channel. Once all the variations have been created
	the input channels are compared to the output channels, and if there is
	an input channel that has no corresponding output channel, or vice versa,
	then a warning is issued that the process will block forever should it 
	attempt to synchronize on that channel. 
	
	Actions can only be child nodes of \code{ActionPrefix} nodes, if an 
	action is guaranteed to block forever, its parent \code{ActionPrefix}
	node is gotten and the process that follows the action has its 
	\code{IsUsed} property set to \texttt{false}. This allows the PLR to skip
	compiling the following process entirely, as it is guaranteed never to run.
	 
	Clearly this analysis is an imprecise approximation of the problem itself. 
	There could be channels that block that the analysis does not detect, because
	it has found a relabeling that causes it to believe that the channel can 
	synchronize, even if the relabeling actually has no chance of being applied
	to that particular channel. Even so, the analysis does detect problems in
	process algebra systems, and is useful for example when channel names or
	relabellings have been mistyped.
	
	It would be interesting to further refine this analysis, for example by 
	traversing through the tree and figuring out exactly which relabeling
	could potentially be applied to which channels. Another possible refinement
	would be to detect cases such as $P = (ch.0+\overline{ch}.0)$. Here the
	input and output channel both exist, however since they are part of the
	same nondeterministic choice only one of them can be chosen and so they
	will never synchronize with each other.

\subsection{Unused Processes}

	\textit{Unused Processes} is an analysis that traverses the syntax tree and
	collects the names of all defined processes, as well as all instances where
	processes are invoked. It then proceeds to check whether all defined 
	processes are invoked at least once. If a process is never invoked a warning 
	is issued to that effect, and if optimizations are turned on the unused 
	process is never compiled at the compilation stage, giving some space 
	savings in the compiled executable.
