\chapter{Final Considerations}

	The first and second chapters of the thesis presented the objectives and 
	goals of the project. They also introduced the subject matter, process 
	algebras, their purpose and structure and what they had in common. The .NET 
	framework was introduced, its history, relation to other virtual 
	machine environments and current status was discussed, as well as the 
	benefits of using virtual machines in general.
	
	The third chapter then documented the design and implementation of the 
	Process Language Runtime itself. The PLR self-compiling syntax tree was 
	explained in detail and reasons given for why that was an optimal design for 
	further extensibility. An overview of the PLR runtime classes was given, as 
	well as an insight into how debugging support is enabled in compiled process 
	language applications. The chapter ended by explaining in detail the 
	structure of a .NET assembly compiled from the PLR syntax tree.
	
	Chapter 4 presented a number of static analyses that are part of the PLR, 
	both classical data flow analyses and more specific analyses for process 
	algebra. How these analyses could be re-used for more than one process 
	algebra, even when they have different constructs, was also explained.
	
	Chapters 5 and 6 documented two separate implementations of process algebras 
	using the PLR, the languages were CCS and KLAIM. The syntax and semantics of 
	the languages was shown, as well as an overview of the most important 
	classes in their implementation. The chapter on KLAIM then explored how 
	additional features that are not included in the PLR could be added to new 
	languages, by using custom syntax tree nodes, PLR compilation events and 
	runtime libraries. Both chapters finished with sizable example systems to 
	give an idea of what a real application of these languages might be.
	
	In chapter seven we looked at a graphical user interface tool that allows
	the user to monitor and interact with running process language applications.
	The challenges faced when integrating with the PLR were discussed as well as
	how the program was kept general enough to work with any process language.
	
	The eighth chapter was about how the CCS language was integrated into a 
	professional development environment, Visual Studio 2008. Some alternative 
	development environments were presented and reasons given for why Visual 
	Studio was chosen. A number of useful features were implemented in the 
	Visual Studio integration, including CCS project support, syntax 
	highlighting, real-time syntax checking, IntelliSense, brace matching and 
	full integration with the debugger and build tool.
	
	Finally, in this chapter related work is discussed, including some 
	alternative implementations of CCS and KLAIM. We explore the potential 
	future work that could be done with the PLR, and end with some concluding 
	remarks.
	
	
\section{Related work}\label{sec:related_work}
	
	There are quite a few other projects that have implemented process algebras, 
	or languages inspired by process algebras, however most of these projects 
	take a different approach than that taken by the PLR. Below is a short 
	summary of some of the notable ones, especially those that focus on CCS or 
	KLAIM.
	
	\textit{KLAVA} \cite{klava} is an implementation of KLAIM. It is a Java 
	library which represents the KLAIM constructs as Java classes. KLAIM 
	applications can then be written in Java using the KLAVA library. The main 
	difference between KLAVA and the PLR implementation of KLAIM (hereafter 
	referred to as PLR KLAIM) is that in KLAVA it is not possible to write the 
	applications using the actual syntax of KLAIM. KLAVA is a much more feature 
	rich implementation of KLAIM than PLR KLAIM, it supports nodes running on 
	different machines, and additional constructs such as non blocking input 
	operations.
	
	\textit{X-Klaim} \cite{xklaim} (which stands for eXtended KLAIM) is another 
	implementation of KLAIM from the authors of KLAVA. It is at a higher level 
	of abstraction than KLAVA and has its own syntax, which is a superset of
	the original KLAIM syntax. X-Klaim code uses the KLAVA library as its 
	runtime library, the X-Klaim compiler compiles X-Klaim code down to Java 
	code that uses the KLAVA library. X-Klaim is similar to PLR KLAIM in that 
	both have a KLAIM syntax and both use a runtime library, the difference is 
	that PLR KLAIM directly emits bytecodes, whereas X-Klaim emits Java source 
	code, which means that X-Klaim code cannot be debugged using the original
	X-Klaim source files. X-Klaim is feature rich and supports many constructs
	not in the original KLAIM.
	
	\textit{AspectK} is an aspect oriented version of KLAIM. Originally 
	introduced in \cite{aspectk}, a full virtual machine for the language was 
	subsequently developed in \cite{giordano}. The KLAIM subset used in AspectK 
	is the same as that used in PLR KLAIM, AspectK then adds aspects on top of 
	that. The difference (aside from the aspect orientation) is that AspectK has 
	its own virtual machine, with bytecodes for common process algebra tasks 
	whereas PLR KLAIM uses an existing virtual machine. An advantage of having a 
	process algebra focused virtual machine is that generated code can be 
	smaller, since each bytecode instruction can perform more work. The PLR does 
	get a similar reduction in code size by generating bytecodes that call 
	methods defined in the PLR runtime library.
	
	\textit{JACK} \cite{jack} is a process algebra implementation written in 
	Java. It is similar to the PLR in that it aims to be a framework that can be
	used for implementing different types of process algebra, although its main
	focus is Communicating Sequential Processes (CSP). The difference is that 
	JACK, like KLAVA, represents algebra constructs as Java classes, and the  
	systems are written using Java code instead of the native syntax of the 
	process algebra being implemented. That is to say, it is a framework, but 
	not a compiler.
	
	CCS has at least two implementations, in \cite{build_ccs_interpreter} a 
	method is presented for how to build a sound CCS interpreter by following 
	the semantics of the language, and \cite{ccs_interpreter} shows how the 
	functional programming language Haskell can be used to build a CCS 
	interpreter with minimal amount of code. Both of these differ from the PLR 
	in that they are interpreters rather than compilers.
	
	None of the above related work aims to do exactly what the PLR attempts, 
	which is to build compilers for process algebras that operate on the 		
	algebra's standard syntax and integrate tightly with an existing virtual 
	machine. The PLR is also the only one of these that explores how existing 
	infrastructure can be used to add features to process algebras, such as 
	allowing CCS to call .NET methods written in another .NET language. A 
	further look at that topic might prove interesting, specifically how it 
	affects the original semantics of the algebra being implemented, what side 
	effects it might produce and what sort of interesting things could be
	modeled in this way.

\section{Further work}
\label{sec:further}

	The PLR could be improved and built upon in several ways. Here we will look 
	at some of them.
	
	\subsection{Additional process algebra constructs}
	
	Perhaps the most obvious improvement to the PLR would be to add support for 
	some of the constructs that are common in process algebras but are not 
	currently included in the PLR. This would make it even simpler to use the 
	PLR as the basis for implementing other process algebras. To get a sense of 
	which constructs would be most beneficial to add, we look shortly at two
	of the most prominent process algebras, CSP and $\pi$-calculus, and 
	what would be needed for them to run on the PLR.
	
	\textit{$\pi$-calculus} was introduced in \cite{Milner89acalculus} and is 
	described by its author as an extension of CCS. There are many variants of 
	$\pi$-calculus with additional features but the core calculus has two 
	noteworthy additions to CCS:
	
	\begin{enumerate}
		\item The \textit{match} construct $[a = b] P$ which compares the values 
		of $a$ and $b$ and behaves as $P$ if they are equal but otherwise turns 
		into the nil process. This is simply a more restricted version of the 
		\texttt{if-then-else} construct which is already implemented in the PLR, 
		the boolean condition is restricted to equality comparison and the 
		\texttt{else} branch is simply the nil process. 
		
		\item The generalization of channel and variable names. In $\pi$-calculus 
		both variable and channel names are seen simply as \textit{names}, and can 
		be passed along channels as data. For example a process $P$ could send the 
		channel name $y$ to process $Q$, which could then send or receive on that 
		channel.
		
	  \begin{align*}
			\mathrm{P} \defeq & \overline{x}y . y(j) . 0\\
			\mathrm{Q} \defeq & x(z) . \overline{z}3 . 0\\
		\end{align*} 
		
		Here we see that $P$ first outputs the name $y$ on channel $x$. Process 
		$Q$ receives the channel name on channel $x$, after which it substitutes
		$z$ with $y$ and becomes $\overline{y}3 \ccsdot 0$. It then sends the value
		3 on the received channel and $P$ accepts it and binds it to the name $j$.
		
		This could fairly easily be added to the PLR. At compilation time each 
		channel name being used could be checked to see whether it was defined as 
		a variable or not. If it had previously been defined as a variable then 
		the synchronization would happen on the channel whose name was stored in 
		the variable, if no variable with that name exists then the name of the 
		channel would be considered a constant. For example in the term $x(z) . 
		\overline{z}3$ we see that $z$ is bound in the first action, and so when 
		we process the $\overline{z}3$ action we know that we should use the name
		stored in $z$ as opposed to the literal name $z$. However in the term 
		$x(z) . \overline{y}3$ we see that $y$ has never been bound as a variable 
		and so when the $\overline{y}3$ action is processed the literal name $y$ 
		is used for the channel.
		
	\end{enumerate}
	
	\textit{CSP} is very similar to CCS. It can synchronize on channels, with or 
	without message passing, it uses action prefixing, restriction, choice and 
	parallel composition. Its definitions of choice and parallel composition are 
	a little bit more complex than those of CCS. It also distinguishes between 
	an inactive process, which is called $STOP$ and is equivalent to the nil 
	process in CCS, and a $SKIP$ process which signals that a process has 
	completed successfully. Finally, as its name \textit{Communicating 
	Sequential Processes} suggests, it offers sequential composition of 
	processes. We now look at how these differences might be implemented in the 
	PLR.
	
	\begin{enumerate}
		\item Implementing the $SKIP$ process is trivial since it does nothing. 
		The main issue is distinguishing it from the $STOP$ process, this can be 
		done by letting them output different text when they are invoked.
		
		\item In CSP a distinction is made between internal non deterministic 
		choice (written $P \sqcap Q$) and external non deterministic choice 
		(written $P\ \square\ Q$). External choice will synchronize with the first 
		event offered by  the environment, so in $a . STOP\ \square\ b . STOP$ it 
		depends on which of $a$ and $b$ is offered first. Internal non 
		deterministic choice however can refuse to participate in an event even 
		though no alternative is offered. The current implementation in the PLR is 
		equivalent to CSP's external choice, if an event (or channel 
		synchronization) is offered on one of the paths then it will be taken. If 
		two or more candidates are offered then the selection between them is made 
		randomly. To enable a simulation of internal choice it could be possible 
		to add some randomness to whether or not an offered channel 
		synchronization is accepted. When the scheduler is finding out which 
		synchronizations are possible it calls a \textsf{CanSyncWith} method on 
		each candidate action. It could be possible to make that call randomly 
		return true or false, which would make sure that a path in an internal 
		choice was not forced to be taken, even if it was the only candidate for 
		synchronization.
		
		\item CSP also handles parallel composition in a slightly different way 
		from CSS. It defines two versions of parallel composition. The first one, 
		which is sometimes called \textit{interleaving} is written as $P 
		\mid\mid\mid Q$. Here $P$ and $Q$ run in parallel and are independent of 
		each other. They can interact but are not forced to. This is equivalent to 
		the PLR's parallel composition construct. 
		
		The other type of parallel composition available in CSP is written as $P 
		\mid\mid_A Q$ or sometimes as $P\ |[\{A\}]|\ Q$. $P$ and $Q$ are said 
		to be \textit{interface parallel}. Here $A$ is a set of channel names that 
		$P$ and $Q$ must synchronize on, e.g. in $P\ |[\{a,b\}]|\ Q$ the 
		processes $P$ and $Q$ must synchronize with each others on channels $a$ 
		and $b$. If one of them has arrived at an $a$ or $b$ action it cannot 
		continue until the other one is ready to synchronize with it. This is 
		similar to parallel composition with restriction in CCS, e.g. $(P \mid Q) 
		\backslash\{a,b\}$. In that expression $P$ and $Q$ have to synchronize on 
		$a$ and $b$ because they are invisible from the outside and so for either 
		process the other process is the only candidate to synchronize with. It is 
		slightly different though because in CSP's version the events $a$ and $b$ 
		are not unobservable from the outside. This could still be implemented 
		much like the CCS expression shown above, the channels $a$ and $b$ would 
		be locally scoped to the $P\ |[\{a,b\}]|\ Q$ process so that no 
		external processes could participate in the synchronization, and the 
		channels would be shown as part of the trace of the system, which would 
		not normally be done in an expression like $(P \mid Q) \backslash\{a,b\}$
	
		\item Sequential composition is written as $P;Q$, it is a process that 
		behaves like $P$ until $P$ terminates and then behaves like $Q$.	This 
		would be easy to implement in the PLR, any finite process will end up as 
		the nil process (or the $STOP$ or $SKIP$ process in CSP), once the nil 
		process has been reached in $P$ a new instance of $Q$ could be 
		instantiated and started. 
	
	\end{enumerate}
	
	In addition to these features, variants of CSP sometimes contain additional 
	features such as interrupts and timeouts which we shall not go into here. 
	
	\subsection{Richer datatypes and expressions}
	
	The initial version of the PLR supports two types of variables, integers and 
	strings. For expressions it supports constants for integers, booleans and 
	strings, as well as simple arithmetic and relational operators for integers 
	and logical operators for booleans. One way to extend the PLR would be to 
	add more support for using other data types from .NET. An example would be 
	to allow passing of .NET objects through channels and calling instance 
	methods on those objects in the receiving process. This would require some 
	additional syntax nodes for the PLR tree, an expression node for 
	constructing a new object and a node for a method call on an object (as 
	opposed to a static method call which is already supported). Other useful 
	features to add might include support for floating point numbers, string 
	formatting and basic string expressions using the + operator.
	
	\subsection{Additional analysis}
	
	Another potential improvement would be to add additional analyses before 
	compilation. This is where the benefit of having a shared syntax tree for 
	multiple algebras becomes apparent, as many of the analyses could be re-used 
	for multiple process algebras (although probably not all of them). This 
	could include common compiler optimization techniques such as constant 
	propagation and Very Busy Expressions analysis, or analyses more directly 
	related to process algebra, such as finding channels that are never used
	and identifying processes that will always block.
	
	\subsection{Bi-similarity of processes}
	One of the interesting things that could be added, for CCS and maybe others, 
	would be an analysis to compare two processes and see if they are 
	\textit{behaviorally equivalent}, also known as \textit{bi-similar}. 
	B-similarity is a congruence, if processes $P$ and $Q$ are bi-similar then 
	it means that if $P$ is a component in a system then it can be replaced with 
	$Q$ and the system will continue to work in the same way, since $P$ and $Q$ 
	exhibit the same behavior. This can for instance be used to write a 
	specification as a simple process expression and then write an 
	implementation for that specification. If the specification and 
	implementation are bi-similar then the implementation is a correct 
	implementation of the specification. An example could be the specification
  \begin{align*}
			\mathrm{CoffeeMachine} \defeq & coin  \ccsdot \out{coffee} \ccsdot CoffeeMachine \\
	\end{align*} and the implementation
  \begin{align*}
			\mathrm{CoffeeMachineImpl} \defeq & ( \mathrm{CoinReceiver} \mid \mathrm{CoffeePourer} ) \backslash \{pour\} \\
			\mathrm{CoinReceiver} \defeq & coin \ccsdot \out{pour} \ccsdot \mathrm{CoinReceiver} \\
			\mathrm{CoffeePourer} \defeq & pour \ccsdot \out{coffee} \ccsdot \mathrm{CoffeePourer}
	\end{align*}	

	Here we see that according to the specification of \textsf{CoffeeMachine} 
	the observable events are an endless stream of $coin$ and $coffee$. This 
	however tells us nothing about how this machine is implemented. The second 
	process \textsf{CoffeeMachineImpl} is the implementation of this coffee 
	machine, it is composed of two components, a receiver for the coins and a 
	component that pours the coffee. They communicate between themselves on the 
	$pour$ channel. Since that channel is hidden (or restricted) it is not 
	observable from the outside, what is observable from the outside is again an 
	endless stream of $coin$ and $coffeee$. In this trivial case it is obvious 
	that \textsf{CoffeeMachineImpl} is a valid implementation of 
	\textsf{CoffeeMachine}.
	
	Bi-similarity can be analyzed by converting a CCS process expression into a 
	\textit{labelled transition system}, which is a state machine where the 
	transitions between states are the actions performed in the process. Weak 
	bi-simulation, or observational equivalence, is perhaps the most interesting 
	bi-simulation to verify. In general terms it states that if $P$ and $Q$ are 
	weakly bi-similar then they will behave exactly the same when observed from 
	the outside, they will offer the same synchronizations or events. However 
	before and after these public events they can perform any number of internal 
	actions or $\tau$ actions which do not have to match between the two 
	processes since they do not affect their behavior as seen from the outside. 
	
	The PLR syntax tree is a rich data structure and would be well suited for 
	this type of analysis. This might not be the type of feature that belongs in 
	a process algebra compiler, instead it might be incorporated into some kind 
	of analysis tool for CCS (or other process algebras) that could make use of 
	the syntax tree of the compiler and the parser and scanner of the CCS 
	compiler. 
	
	This section has only briefly touched on the possibilities of verifying 
	process behavior using bi-simulation, for a more comprehensive explanation 
	see \cite{reactive}.
	
\section{Conclusions}

	The initial goal of this project was to explore how process algebras could 
	fit in with the .NET framework and how the constructs these algebras had in 
	common could be abstracted into common building blocks that could be re-used 
	in many implementations. The implementations of two process algebras using 
	those building blocks was necessary to prove their generality. Integrating a 
	process algebra into Visual Studio was more of an afterthought, but once I 
	started on it I saw how useful it was, and how much more enjoyable it was to 
	write CCS in this integrated environment, and so I was inspired to explore 
	exactly how well the language could be integrated and what services were 
	available.
	
	After finishing this project I believe that the .NET framework is a
	good platform for implementing process algebras. The main benefit is the 
	ease of interacting with other .NET languages. It is easy to add additional 
	features to a language by writing runtime libraries in languages like C\# 
	and it is easy to make a process algebra call into any arbitrary .NET 
	assembly. Another great benefit is being able to debug the compiled 
	applications, and in general great tool support in programs like Visual 
	Studio. The built in API to emit bytecode is well structured and easy to 
	use, and the bytecode itself is well designed and surprisingly readable once 
	you have gotten used to it. 
	
	There are downsides to .NET as well. The only real support for concurrency 
	is by using threads. I had thought beforehand that there might be some low 
	level support for that in the actual bytecode but that was not the case. 
	Threading can only be done through the standard class library, using the 
	\code{Thread} class, which shows that the class library is at least as 
	important as the virtual machine itself. It is also my conclusion that while
	.NET is a good platform for implementing process algebras, it is not 
	\textit{specifically} good for process algebras, it is more that it is a 
	good platform for implementing programming languages in general. 

	The fact that there exist other implementations of the CLI (Common Language 
	Infrastructure) specification is very useful. The PLR library, CCS compiler 
	and KLAIM compiler all work flawlessly on Mono, the CLI implementation that 
	runs on the Linux and Unix family of operating systems. The compiled process 
	applications also work on Mono without problems. I had expected that the 
	compilers and the PLR would work, since they are command line tools/libraries
	and do not depend on the underlying operating system much, except for reading
	and writing files. What I did not expect was that the Process Viewer 
	application would work, since it has a graphical user interface which uses
	the underlying graphic system of the Windows operating system. To my surprise
	the Process Viewer ran without any problems on Mono the first time I tried 
	it. It is worth noting that no special consideration was needed to support 
	Mono, I simply wrote the whole thing using .NET on Windows and once it was 
	ready I compiled and ran it with Mono 2.4.2 on an Ubuntu Linux 8.10 
	operating system and it worked flawlessly. This means that the PLR is truly 
	cross-platform, which should make it accessible to more people, especially 
	since many researchers in computer science do not use the Windows operating 
	system at all. 

	Building re-usable components for process algebras went very well. The PLR 
	takes care of a lot of the basic work that anyone implementing a process 
	algebra would otherwise have to do themselves. Not just the basic process 
	algebra constructs themselves, but also expression trees for arithmetic 
	expressions, static analyses, emitting debugging symbols, and helper methods 
	for calling into .NET code. An implementor can use this and focus his energy 
	on what matters, implementing specific constructs and emitting the bytecodes 
	for them.
	
	Overall I consider the project a success, as it has resulted in two working 
	process algebra implementations (one of which is fully integrated into 
	Visual Studio), a graphical tool for working with compiled process language 
	applications, as well as the main software product: a library/compiler that 
	can (and hopefully will) be used by future implementers of other process 
	algebras.